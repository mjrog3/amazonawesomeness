{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports**\n",
    "\n",
    "Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "current_folder = os.getcwd()\n",
    "CDs_folder = 'CDs_and_Vinyl'\n",
    "\n",
    "# Open and load json training files\n",
    "x = pd.read_json(os.path.join(current_folder, CDs_folder, 'train', 'review_training.json'))\n",
    "y = pd.read_json(os.path.join(current_folder, CDs_folder, 'train', 'product_training.json'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk import sentiment\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>vote</th>\n",
       "      <th>verified</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>style</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6673F1740E03573BCD64238FE711FC69</td>\n",
       "      <td>9C856D4A18E1355783B3B28B7ECC1848</td>\n",
       "      <td>1451520000</td>\n",
       "      <td>9</td>\n",
       "      <td>True</td>\n",
       "      <td>12 31, 2015</td>\n",
       "      <td>{'Format:': ' MP3 Music'}</td>\n",
       "      <td>8D88BB79AAC50277AEE82FCFD77F6744</td>\n",
       "      <td>Finding the Beatles all over again - and bette...</td>\n",
       "      <td>I sit listening - with my jaw to the floor - H...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>690819436E20BB31657AF6B58B984DD4</td>\n",
       "      <td>6E9ABBD26A27C2B2851D1EC34A01CBDC</td>\n",
       "      <td>1113523200</td>\n",
       "      <td>9</td>\n",
       "      <td>False</td>\n",
       "      <td>04 15, 2005</td>\n",
       "      <td>{'Format:': ' Audio CD'}</td>\n",
       "      <td>8DC0611245A871AC51BBEEBB85F33A58</td>\n",
       "      <td>These guys can sing!  Such classic tunes...poi...</td>\n",
       "      <td>Under Appreciated....</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A680D4753F0CEA2252C168A6ACB2B623</td>\n",
       "      <td>B637C3C93E61094474710F456928BE9F</td>\n",
       "      <td>1126137600</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>09 8, 2005</td>\n",
       "      <td>None</td>\n",
       "      <td>2259386624CFA0EC53A75A50A9BB57A5</td>\n",
       "      <td>Snoop Doggy Dogg made a classic album, DoggySt...</td>\n",
       "      <td>DoggyStyle</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F4A966F1FA340B16651D676BC246D227</td>\n",
       "      <td>AA7918E9410D650A076221C7B2934A09</td>\n",
       "      <td>954979200</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>04 6, 2000</td>\n",
       "      <td>{'Format:': ' Audio CD'}</td>\n",
       "      <td>7A65A155C993535BC99CBCB39E7161B5</td>\n",
       "      <td>Stevie Nicks Has had Her Moments. I Like Some ...</td>\n",
       "      <td>Pretty Good but a Bit Dated</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EF59DAF0C00319A48D4784266FD157EE</td>\n",
       "      <td>2293C9B7950A3356B95828419A677720</td>\n",
       "      <td>1477958400</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>11 1, 2016</td>\n",
       "      <td>{'Format:': ' Audio CD'}</td>\n",
       "      <td>C69A09446009C500B1364B7DB5510497</td>\n",
       "      <td>Great cd.</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>770781</th>\n",
       "      <td>BD91503308A437374C3254EDC8BC24CB</td>\n",
       "      <td>936ED23AF4D23943786BBD44D0F1114B</td>\n",
       "      <td>1136246400</td>\n",
       "      <td>12</td>\n",
       "      <td>True</td>\n",
       "      <td>01 3, 2006</td>\n",
       "      <td>{'Format:': ' Audio CD'}</td>\n",
       "      <td>AAB61000438939C8E6165CFCCF02A488</td>\n",
       "      <td>This was the first Simple Minds album that I b...</td>\n",
       "      <td>Their Most Fully Realized Artistic Studio Stat...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>770782</th>\n",
       "      <td>9BC50277D18FAB423AD33C8CE4CC000D</td>\n",
       "      <td>EF922377A87E9D01F50065F2DA1722A8</td>\n",
       "      <td>1290556800</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>11 24, 2010</td>\n",
       "      <td>None</td>\n",
       "      <td>0441BC4F6B7BD180769FDCDD8E603560</td>\n",
       "      <td>I have owned a CD copy of this show for at lea...</td>\n",
       "      <td>Forgettable R&amp;H</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>770783</th>\n",
       "      <td>4062627CA1586E517520483964299349</td>\n",
       "      <td>E1F0B0EBC6A36F33301E4FD0B3D62D52</td>\n",
       "      <td>1311120000</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>07 20, 2011</td>\n",
       "      <td>{'Format:': ' Audio CD'}</td>\n",
       "      <td>8AB3EEBF23F3583A4396A57DB291D548</td>\n",
       "      <td>carnival of souls to me is bad i gave my cd aw...</td>\n",
       "      <td>Darren d.</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>770784</th>\n",
       "      <td>0AE44A6A9176E6A52507B6ABDDA80B00</td>\n",
       "      <td>DDDC81E6B8C3F8C91867F9AECB385135</td>\n",
       "      <td>1111968000</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>03 28, 2005</td>\n",
       "      <td>{'Format:': ' Audio CD'}</td>\n",
       "      <td>51B2E62E0A5864316BDB33FD4A729B37</td>\n",
       "      <td>This is an awesome slayer album. I love the th...</td>\n",
       "      <td>awesome slayer cd</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>770785</th>\n",
       "      <td>E4CDE9C72DB665961BE969B082806619</td>\n",
       "      <td>27FDC1E110DB45E1A7E0CCEABECF8632</td>\n",
       "      <td>1105401600</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>01 11, 2005</td>\n",
       "      <td>{'Format:': ' Audio CD'}</td>\n",
       "      <td>A7487BE472C2B632FCE5F097B47F5DE0</td>\n",
       "      <td>Smile is Brian's gift to a sick and troubled w...</td>\n",
       "      <td>Music just doesn't get any better...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>770786 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    asin                        reviewerID   \n",
       "0       6673F1740E03573BCD64238FE711FC69  9C856D4A18E1355783B3B28B7ECC1848  \\\n",
       "1       690819436E20BB31657AF6B58B984DD4  6E9ABBD26A27C2B2851D1EC34A01CBDC   \n",
       "2       A680D4753F0CEA2252C168A6ACB2B623  B637C3C93E61094474710F456928BE9F   \n",
       "3       F4A966F1FA340B16651D676BC246D227  AA7918E9410D650A076221C7B2934A09   \n",
       "4       EF59DAF0C00319A48D4784266FD157EE  2293C9B7950A3356B95828419A677720   \n",
       "...                                  ...                               ...   \n",
       "770781  BD91503308A437374C3254EDC8BC24CB  936ED23AF4D23943786BBD44D0F1114B   \n",
       "770782  9BC50277D18FAB423AD33C8CE4CC000D  EF922377A87E9D01F50065F2DA1722A8   \n",
       "770783  4062627CA1586E517520483964299349  E1F0B0EBC6A36F33301E4FD0B3D62D52   \n",
       "770784  0AE44A6A9176E6A52507B6ABDDA80B00  DDDC81E6B8C3F8C91867F9AECB385135   \n",
       "770785  E4CDE9C72DB665961BE969B082806619  27FDC1E110DB45E1A7E0CCEABECF8632   \n",
       "\n",
       "        unixReviewTime  vote  verified   reviewTime   \n",
       "0           1451520000     9      True  12 31, 2015  \\\n",
       "1           1113523200     9     False  04 15, 2005   \n",
       "2           1126137600  None     False   09 8, 2005   \n",
       "3            954979200  None     False   04 6, 2000   \n",
       "4           1477958400  None      True   11 1, 2016   \n",
       "...                ...   ...       ...          ...   \n",
       "770781      1136246400    12      True   01 3, 2006   \n",
       "770782      1290556800  None     False  11 24, 2010   \n",
       "770783      1311120000     3     False  07 20, 2011   \n",
       "770784      1111968000     2     False  03 28, 2005   \n",
       "770785      1105401600  None     False  01 11, 2005   \n",
       "\n",
       "                            style                      reviewerName   \n",
       "0       {'Format:': ' MP3 Music'}  8D88BB79AAC50277AEE82FCFD77F6744  \\\n",
       "1        {'Format:': ' Audio CD'}  8DC0611245A871AC51BBEEBB85F33A58   \n",
       "2                            None  2259386624CFA0EC53A75A50A9BB57A5   \n",
       "3        {'Format:': ' Audio CD'}  7A65A155C993535BC99CBCB39E7161B5   \n",
       "4        {'Format:': ' Audio CD'}  C69A09446009C500B1364B7DB5510497   \n",
       "...                           ...                               ...   \n",
       "770781   {'Format:': ' Audio CD'}  AAB61000438939C8E6165CFCCF02A488   \n",
       "770782                       None  0441BC4F6B7BD180769FDCDD8E603560   \n",
       "770783   {'Format:': ' Audio CD'}  8AB3EEBF23F3583A4396A57DB291D548   \n",
       "770784   {'Format:': ' Audio CD'}  51B2E62E0A5864316BDB33FD4A729B37   \n",
       "770785   {'Format:': ' Audio CD'}  A7487BE472C2B632FCE5F097B47F5DE0   \n",
       "\n",
       "                                               reviewText   \n",
       "0       Finding the Beatles all over again - and bette...  \\\n",
       "1       These guys can sing!  Such classic tunes...poi...   \n",
       "2       Snoop Doggy Dogg made a classic album, DoggySt...   \n",
       "3       Stevie Nicks Has had Her Moments. I Like Some ...   \n",
       "4                                               Great cd.   \n",
       "...                                                   ...   \n",
       "770781  This was the first Simple Minds album that I b...   \n",
       "770782  I have owned a CD copy of this show for at lea...   \n",
       "770783  carnival of souls to me is bad i gave my cd aw...   \n",
       "770784  This is an awesome slayer album. I love the th...   \n",
       "770785  Smile is Brian's gift to a sick and troubled w...   \n",
       "\n",
       "                                                  summary image  \n",
       "0       I sit listening - with my jaw to the floor - H...  None  \n",
       "1                                   Under Appreciated....  None  \n",
       "2                                              DoggyStyle  None  \n",
       "3                             Pretty Good but a Bit Dated  None  \n",
       "4                                              Five Stars  None  \n",
       "...                                                   ...   ...  \n",
       "770781  Their Most Fully Realized Artistic Studio Stat...  None  \n",
       "770782                                    Forgettable R&H  None  \n",
       "770783                                          Darren d.  None  \n",
       "770784                                  awesome slayer cd  None  \n",
       "770785               Music just doesn't get any better...  None  \n",
       "\n",
       "[770786 rows x 11 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PATCH TO SPEED UP (ONLY IF INTEL CHIP) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    }
   ],
   "source": [
    "from sklearnex import patch_sklearn \n",
    "\n",
    "patch_sklearn()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x #770786 rows <- # reviews\n",
    "y #073082 rows <- # products"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature creation "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making a sample set to test features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ts = test sample 2 thousand reviews long\n",
    "ts = x[:2000]\n",
    "ts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropUnverified = x[x.verified == True]\n",
    "reviewCount = x.groupby('asin')[\"reviewerID\"].count()\n",
    "reviewCount = reviewCount.rename(\"Review_Count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropUnverified.groupby('asin')['unixReviewTime'].mean() #63401 rows! -> some products don't have ANY verified reviews"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Percent verified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_verified = dropUnverified.groupby(\"asin\")[\"reviewerID\"].count()\n",
    "percent_verified = percent_verified/reviewCount\n",
    "percent_verified = percent_verified.apply(lambda x: x if x > 0  else 0)\n",
    "percent_verified = percent_verified.rename(\"%_Verified\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping bots by time of post\n",
    "\n",
    "All the comments are encoded with the date only, no time, so this is not possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "droppedBots = x[x.unixReviewTime % 100 != 0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total number of votes across reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_copy = x.copy(deep=True)\n",
    "total_review_num = x_copy.vote\n",
    "total_review_num = total_review_num.apply(lambda x: float(x.replace(\",\", \"\")) if type(x) == str  else 0)\n",
    "total_review_num = total_review_num.rename(\"vote\").to_frame()\n",
    "x_copy[\"vote\"] = total_review_num[\"vote\"]\n",
    "total_votes = x_copy.groupby('asin')[\"vote\"].sum(\"vote\")\n",
    "total_votes = total_votes.rename(\"Total_Votes\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Percent of reviews with images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_image = x[x.image.astype('string') != \"None\"]\n",
    "with_image = with_image.groupby('asin')[\"image\"].count()\n",
    "with_image_percentage = with_image/reviewCount\n",
    "with_image_percentage = with_image_percentage.apply(lambda x: x if x > 0  else 0)\n",
    "#with_image_percentage = with_image_percentage[with_image_percentage > 0] # - to see how many reviews have at least one image\n",
    "with_image_percentage = with_image_percentage.rename(\"%_Image\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Small group to test text analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdata = x.groupby(\"asin\").get_group(\"0000B049F5B33CD310EB1AB236E20191\")\n",
    "\n",
    "# np.mean(len(str(testdata[\"reviewText\"]).split()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Length of Reviews Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewlength = x.groupby('asin')[\"reviewText\"].apply(lambda x: x.str.split().str.len().mean())\n",
    "reviewlength = reviewlength.fillna(0).rename(\"Review_Length\")\n",
    "\n",
    "summarylength = x.groupby('asin')[\"summary\"].apply(lambda x: x.str.split().str.len().mean())\n",
    "summarylength = summarylength.fillna(0).rename(\"Summary_Length\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a= 1\n",
    "\n",
    "def f(votes):\n",
    "    if (votes>a):\n",
    "        return 2\n",
    "    return 344"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'to_string'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12992/3629067703.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0msia\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msentiment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSentimentIntensityAnalyzer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mRSentiment\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"reviewText\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0msia\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolarity_scores\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"compound\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mRSentiment\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRSentiment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Review_sentiment\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\zachg\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4629\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat64\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4630\u001b[0m         \"\"\"\n\u001b[1;32m-> 4631\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mSeriesApply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert_dtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4632\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4633\u001b[0m     def _reduce(\n",
      "\u001b[1;32mc:\\Users\\zachg\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1023\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1024\u001b[0m         \u001b[1;31m# self.f is Callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1025\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1026\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1027\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\zachg\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1074\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1075\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1076\u001b[1;33m                 mapped = lib.map_infer(\n\u001b[0m\u001b[0;32m   1077\u001b[0m                     \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1078\u001b[0m                     \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\zachg\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\_libs\\lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12992/3629067703.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0msia\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msentiment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSentimentIntensityAnalyzer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mRSentiment\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"reviewText\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0msia\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolarity_scores\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"compound\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mRSentiment\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRSentiment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Review_sentiment\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'to_string'"
     ]
    }
   ],
   "source": [
    "x=x[:200]\n",
    "sia = sentiment.SentimentIntensityAnalyzer()\n",
    "RSentiment = x[\"reviewText\"].apply(lambda x: sia.polarity_scores(x.to_string())[\"compound\"]) #sentiment per no avg\n",
    "RSentiment = RSentiment.fillna(0).rename(\"Review_sentiment\") #make that col\n",
    "#apply f(votes) to each element\n",
    "#avg the result\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Percentage Uppercase Feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def count_cap(str):\n",
    "#     stripped = ''.join(str.split())\n",
    "#     return sum(c.isupper() for c in ''.join(stripped))/len(stripped)\n",
    "\n",
    "RpercentCap = x.groupby('asin')[\"reviewText\"].apply(lambda x: (x.str.count(\"[A-Z]\")/x.str.len()).mean())\n",
    "RpercentCap = RpercentCap.fillna(0).rename(\"Review_Percent_Uppercase\")\n",
    "\n",
    "SpercentCap = x.groupby('asin')[\"summary\"].apply(lambda x: (x.str.count(\"[A-Z]\")/x.str.len()).mean())\n",
    "SpercentCap = SpercentCap.fillna(0).rename(\"Summary_Percent_Uppercase\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actual Awesomeness Feature: # times the word \"awesome\" is used per review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actualAwesomeness = x.groupby('asin')[\"reviewText\"].apply(lambda x: (x.str.count(\"[Aa]wesome\")).mean())\n",
    "actualAwesomeness = actualAwesomeness.fillna(0).rename(\"Actual_Awesomeness\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helper function for sentiment analysis testing\n",
    "def review_sentiment(text):\n",
    "    sia = sentiment.SentimentIntensityAnalyzer()\n",
    "    return sia.polarity_scores(text)[\"compound\"]\n",
    "\n",
    "## just some examples to gut check\n",
    "# print(review_sentiment(\"Ugh, what a terrible, horrible, no good, very bad day\"))\n",
    "# print(review_sentiment(\"I went to the store today\"))\n",
    "# print(review_sentiment(\"I absolutely love my favorite best friend\"))\n",
    "\n",
    "import string\n",
    "def preprocess(text):\n",
    "    try:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        text = \"\".join([char for char in text.lower() if char not in string.punctuation])\n",
    "        words = [word for word in word_tokenize(text) if word not in stopwords.words(\"english\")]\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in words]\n",
    "        return ' '.join(tokens)\n",
    "    except:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[\"processedTest\"] = x[\"reviewText\"].apply(preprocess)\n",
    "x[\"processedTest\"].to_json(\"../processedReviews.json\")\n",
    "x[\"processedSums\"] = x[\"summary\"].apply(preprocess)\n",
    "x[\"processedSums\"].to_json(\"../processedSummary.json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply sentiment analysis to data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sia = sentiment.SentimentIntensityAnalyzer()\n",
    "RavgSentiment = x.groupby('asin')[\"processedTest\"].fillna(\"\").apply(lambda x: np.mean(sia.polarity_scores(x)[\"compound\"]))\n",
    "RavgSentiment = RavgSentiment.rename(\"Review_Avg_Sentiment\")\n",
    "\n",
    "sia = sentiment.SentimentIntensityAnalyzer()\n",
    "SavgSentiment = x.groupby('asin')[\"processedSums\"].fillna(\"\").apply(lambda x: np.mean(sia.polarity_scores(x)[\"compound\"]))\n",
    "SavgSentiment = RavgSentiment.rename(\"Summary_Avg_Sentiment\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = pd.read_json(\"../processedReviews.json\",typ='series')\n",
    "text.to_frame('processedText')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(max_features=10)\n",
    "\n",
    "x[\"processedText\"] = text\n",
    "tfidf_DF = x[[\"asin\",\"processedText\"]]\n",
    "\n",
    "\n",
    "\n",
    "# tfidf_DF[\"processedText\"] = \n",
    "# tfidf_DF[\"processedTest\"] = tfidf_DF[\"processedTest\"].fillna(\"\")\n",
    "# tfidf_DF[\"processedRevs\"] = tfidf_DF[\"reviewText\"].fillna(\"\").apply(preprocess)\n",
    "\n",
    "tfidf.fit_transform(tfidf_DF[\"processedText\"])\n",
    "# tfidffeatures.toarray()\n",
    "feature_names = tfidf.get_feature_names()\n",
    "tfidfarray = tfidf.transform(tfidf_DF[\"processedText\"]).toarray()\n",
    "\n",
    "featuredf = pd.DataFrame(tfidfarray,columns = feature_names)\n",
    "tfidf_DF = pd.concat([tfidf_DF,featuredf],axis=1)\n",
    "feature = tfidf_DF.groupby(\"asin\")[feature_names].mean()\n",
    "\n",
    "feature\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing prep\n",
    "\n",
    "Feature vectors must have format: col 1 as 'asin'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently using features:\n",
    "\n",
    "Name                    |     Column Name\n",
    "\n",
    "reviewCount                    Review_Count\n",
    "\n",
    "~~with_image_percentage          %_Image~~\n",
    "\n",
    "percent_verified               %_Verified\n",
    "\n",
    "total_votes                    Total_Votes\n",
    "\n",
    "reviewlength                   Review_Length\n",
    "\n",
    "summarylength                  Summary_Length\n",
    "\n",
    "RpercentCap                    Review_%_Uppercase\n",
    "\n",
    "SpercentCap                    Summary_%_Uppercase\n",
    "\n",
    "~~actualAwesomeness              Actual_Awesomeness~~\n",
    "\n",
    "RavgSentiment                  Review_Avg_Sentiment\n",
    "\n",
    "SavgSentiment                  Summary_Avg_Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine all individual features into one dataFrame\n",
    "\n",
    "#enter any features to be combined here! \n",
    "#   They must be pd dataFrames with the 'asin' column for this to work\n",
    "features = [reviewCount,percent_verified,total_votes,RavgSentiment,SavgSentiment,reviewlength,summarylength,RpercentCap,SpercentCap]\n",
    "\n",
    "z = x['asin']\n",
    "for f in features:\n",
    "    z = pd.merge(z,f,'inner','asin')\n",
    "\n",
    "#combine resultant data with correct answers \n",
    "temp = pd.merge(z,y,'inner','asin') \n",
    "temp = temp.groupby(\"asin\").mean()\n",
    "\n",
    "\n",
    "#split into features (x) and awesomeness (y), which now row correspond\n",
    "merged_x = temp.drop(['awesomeness'], axis=1)\n",
    "merged_y = temp[\"awesomeness\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review_Count</th>\n",
       "      <th>%_Verified</th>\n",
       "      <th>Total_Votes</th>\n",
       "      <th>Review_Avg_Sentiment</th>\n",
       "      <th>Summary_Avg_Sentiment</th>\n",
       "      <th>Review_Length</th>\n",
       "      <th>Summary_Length</th>\n",
       "      <th>Review_Percent_Uppercase</th>\n",
       "      <th>Summary_Percent_Uppercase</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>asin</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0000B049F5B33CD310EB1AB236E20191</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.9233</td>\n",
       "      <td>0.8902</td>\n",
       "      <td>88.500000</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>0.210796</td>\n",
       "      <td>0.503689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00018184A9EC4D270219A296B2580303</th>\n",
       "      <td>18.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>45.0</td>\n",
       "      <td>0.8327</td>\n",
       "      <td>0.9619</td>\n",
       "      <td>235.222222</td>\n",
       "      <td>5.166667</td>\n",
       "      <td>0.076743</td>\n",
       "      <td>0.115941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000281A9CAC43FF1F335726A390636DA</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.4019</td>\n",
       "      <td>-0.0516</td>\n",
       "      <td>130.500000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>0.086681</td>\n",
       "      <td>0.153030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00030884DF109F325638A6BFD5B13CFF</th>\n",
       "      <td>29.0</td>\n",
       "      <td>0.586207</td>\n",
       "      <td>83.0</td>\n",
       "      <td>0.9969</td>\n",
       "      <td>0.9945</td>\n",
       "      <td>79.655172</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.059310</td>\n",
       "      <td>0.121813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000325BA25966B5FC701D5D2B5DBA4E0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.8257</td>\n",
       "      <td>-0.3840</td>\n",
       "      <td>91.500000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.038202</td>\n",
       "      <td>0.124301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00039B53F332D3A911B0B18F88051C80</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.9201</td>\n",
       "      <td>0.8122</td>\n",
       "      <td>155.666667</td>\n",
       "      <td>4.833333</td>\n",
       "      <td>0.040205</td>\n",
       "      <td>0.102725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000449E7E71585B3F1A7EAE8B654B468</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>160.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.011419</td>\n",
       "      <td>0.022222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0004D01A4CED3FE007D35FB3933B3A6C</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.8858</td>\n",
       "      <td>0.7133</td>\n",
       "      <td>347.000000</td>\n",
       "      <td>5.800000</td>\n",
       "      <td>0.036524</td>\n",
       "      <td>0.053700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00055F6EC779D818B9F33AA0885FC6E3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.9201</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>160.500000</td>\n",
       "      <td>11.500000</td>\n",
       "      <td>0.061778</td>\n",
       "      <td>0.090441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000577BC760B4C7BD980939F0CB41F65</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.9451</td>\n",
       "      <td>-0.3182</td>\n",
       "      <td>121.250000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.028575</td>\n",
       "      <td>0.151552</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Review_Count  %_Verified  Total_Votes   \n",
       "asin                                                                      \n",
       "0000B049F5B33CD310EB1AB236E20191           4.0    0.500000          6.0  \\\n",
       "00018184A9EC4D270219A296B2580303          18.0    0.166667         45.0   \n",
       "000281A9CAC43FF1F335726A390636DA           4.0    0.750000          5.0   \n",
       "00030884DF109F325638A6BFD5B13CFF          29.0    0.586207         83.0   \n",
       "000325BA25966B5FC701D5D2B5DBA4E0           4.0    1.000000          7.0   \n",
       "00039B53F332D3A911B0B18F88051C80           6.0    0.666667         27.0   \n",
       "000449E7E71585B3F1A7EAE8B654B468           2.0    0.000000         30.0   \n",
       "0004D01A4CED3FE007D35FB3933B3A6C           5.0    0.400000         29.0   \n",
       "00055F6EC779D818B9F33AA0885FC6E3           4.0    0.750000         24.0   \n",
       "000577BC760B4C7BD980939F0CB41F65           4.0    0.750000         11.0   \n",
       "\n",
       "                                  Review_Avg_Sentiment  Summary_Avg_Sentiment   \n",
       "asin                                                                            \n",
       "0000B049F5B33CD310EB1AB236E20191                0.9233                 0.8902  \\\n",
       "00018184A9EC4D270219A296B2580303                0.8327                 0.9619   \n",
       "000281A9CAC43FF1F335726A390636DA                0.4019                -0.0516   \n",
       "00030884DF109F325638A6BFD5B13CFF                0.9969                 0.9945   \n",
       "000325BA25966B5FC701D5D2B5DBA4E0                0.8257                -0.3840   \n",
       "00039B53F332D3A911B0B18F88051C80                0.9201                 0.8122   \n",
       "000449E7E71585B3F1A7EAE8B654B468                0.0000                 0.4601   \n",
       "0004D01A4CED3FE007D35FB3933B3A6C                0.8858                 0.7133   \n",
       "00055F6EC779D818B9F33AA0885FC6E3                0.9201                 0.0000   \n",
       "000577BC760B4C7BD980939F0CB41F65                0.9451                -0.3182   \n",
       "\n",
       "                                  Review_Length  Summary_Length   \n",
       "asin                                                              \n",
       "0000B049F5B33CD310EB1AB236E20191      88.500000        4.500000  \\\n",
       "00018184A9EC4D270219A296B2580303     235.222222        5.166667   \n",
       "000281A9CAC43FF1F335726A390636DA     130.500000        3.500000   \n",
       "00030884DF109F325638A6BFD5B13CFF      79.655172        5.000000   \n",
       "000325BA25966B5FC701D5D2B5DBA4E0      91.500000        3.000000   \n",
       "00039B53F332D3A911B0B18F88051C80     155.666667        4.833333   \n",
       "000449E7E71585B3F1A7EAE8B654B468     160.000000        6.000000   \n",
       "0004D01A4CED3FE007D35FB3933B3A6C     347.000000        5.800000   \n",
       "00055F6EC779D818B9F33AA0885FC6E3     160.500000       11.500000   \n",
       "000577BC760B4C7BD980939F0CB41F65     121.250000        3.000000   \n",
       "\n",
       "                                  Review_Percent_Uppercase   \n",
       "asin                                                         \n",
       "0000B049F5B33CD310EB1AB236E20191                  0.210796  \\\n",
       "00018184A9EC4D270219A296B2580303                  0.076743   \n",
       "000281A9CAC43FF1F335726A390636DA                  0.086681   \n",
       "00030884DF109F325638A6BFD5B13CFF                  0.059310   \n",
       "000325BA25966B5FC701D5D2B5DBA4E0                  0.038202   \n",
       "00039B53F332D3A911B0B18F88051C80                  0.040205   \n",
       "000449E7E71585B3F1A7EAE8B654B468                  0.011419   \n",
       "0004D01A4CED3FE007D35FB3933B3A6C                  0.036524   \n",
       "00055F6EC779D818B9F33AA0885FC6E3                  0.061778   \n",
       "000577BC760B4C7BD980939F0CB41F65                  0.028575   \n",
       "\n",
       "                                  Summary_Percent_Uppercase  \n",
       "asin                                                         \n",
       "0000B049F5B33CD310EB1AB236E20191                   0.503689  \n",
       "00018184A9EC4D270219A296B2580303                   0.115941  \n",
       "000281A9CAC43FF1F335726A390636DA                   0.153030  \n",
       "00030884DF109F325638A6BFD5B13CFF                   0.121813  \n",
       "000325BA25966B5FC701D5D2B5DBA4E0                   0.124301  \n",
       "00039B53F332D3A911B0B18F88051C80                   0.102725  \n",
       "000449E7E71585B3F1A7EAE8B654B468                   0.022222  \n",
       "0004D01A4CED3FE007D35FB3933B3A6C                   0.053700  \n",
       "00055F6EC779D818B9F33AA0885FC6E3                   0.090441  \n",
       "000577BC760B4C7BD980939F0CB41F65                   0.151552  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_x[:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "\n",
    "(choose one)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "#kernel type\n",
    "#‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’\n",
    "kernel = 'poly'\n",
    "\n",
    "#if kernel type = 'poly', degree of poly\n",
    "degree = 4\n",
    "\n",
    "#whether to enable probability estimates (will slow it down a lot)\n",
    "probability = False\n",
    "\n",
    "#tau - penalty for errors is inversely proportional to C\n",
    "C = 0.75\n",
    "\n",
    "param_grid = [{'kernel': ['poly','rbf','sigmoid'],\n",
    "             'C': [0.25,0.5,0.75,1]}]\n",
    "\n",
    "# param_grid = [{'kernel': ['poly','rbf','sigmoid'],\n",
    "#             'C': [0.25,0.5,0.75,1]}]\n",
    "# -> {'C': 0.75, 'kernel': 'poly'}\n",
    "# f1: 0.694757392846268\n",
    "\n",
    "# param_grid = [{'degree': [2,3,4,5],\n",
    "#              'C': [0.7,0.75,0.8]}]\n",
    "# -> {'C': 0.75, 'degree': 4}\n",
    "# f1: 0.6805724352995487\n",
    "\n",
    "svm_model = svm.SVC(C=C, kernel=kernel, degree=degree, probability=probability)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "model = GaussianNB()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "#choose from 'entropy', 'gini', 'log_loss'\n",
    "criterion = 'gini'\n",
    "\n",
    "    ###STOPPING CONDITIONS\n",
    "#max height/depth of the tree OR None if no limit\n",
    "max_depth = 5\n",
    "\n",
    "#if sat(N) < n_samples*this, don't try to split it any further\n",
    "min_samples_split = .05\n",
    "\n",
    "    ###OTHER\n",
    "#if min(sat(nodes after a split)) < n_samples*this, split not considered \n",
    "#       (aka model isnt allowed to create leaf nodes with < n_samples*this samples)\n",
    "#       (smooths the model, avoids splits like 2000 -> 1999 vs 1)\n",
    "min_samples_leaf = 0.01\n",
    "\n",
    "# param_grid = [{'criterion': ['entropy','gini','log_loss'],\n",
    "#             'max_depth': [None,5,10,15,20],\n",
    "#             'min_samples_split': [2,10,20,0.01,0.05,0.1,0.2],\n",
    "#             'min_samples_leaf':[2,10,20,0.01,0.05,0.1,0.2]}]\n",
    "# -> {'criterion': 'gini', 'max_depth': 5, 'min_samples_leaf': 0.01, 'min_samples_split': 0.05}\n",
    "# f1: 0.6440347415923986\n",
    "\n",
    "dtree_model = tree.DecisionTreeClassifier(criterion=criterion, max_depth=max_depth, min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree - See what it looks like! \n",
    "\n",
    "(only run if the model is a decision tree)\n",
    "\n",
    "*note: this trains the model on the whole dataset, so don't do before testing!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree_model.fit(merged_x,merged_y)\n",
    "tree.plot_tree(dtree_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#number of neighbors\n",
    "n_neighbors = 2500\n",
    "\n",
    "#do you want proximity to effect influence neighbors have or not\n",
    "# 'uniform' or 'distance'\n",
    "weights='distance'\n",
    "\n",
    "#leaf size passed to trees\n",
    "leaf_size=3\n",
    "\n",
    "# param_grid = [{'n_neighbors': [5,7,9,11],\n",
    "#             'weights': ['uniform','distance'],\n",
    "#             'leaf_size':[15,20,25,30]}] -> f1: 0.67, {'leaf_size': 15, 'n_neighbors': 11, 'weights': 'uniform'}\n",
    "\n",
    "# param_grid = [{'n_neighbors': [9,11,15,19,23],\n",
    "#              'weights': ['uniform','distance'],\n",
    "#              'leaf_size':[5,10,15,20]}] -> f1: 0.65, {'leaf_size': 5, 'n_neighbors': 23, 'weights': 'uniform'}\n",
    "\n",
    "# param_grid = [{'n_neighbors': [11,22,33,44,55],\n",
    "#              'weights': ['uniform','distance'],\n",
    "#              'leaf_size':[1,2,3,4,5,6,7,8,9,10,11,12]}] \n",
    "# -> {'leaf_size': 1, 'n_neighbors': 55, 'weights': 'uniform'}\n",
    "# f1: 0.63\n",
    "\n",
    "# param_grid = [{'n_neighbors': [11,22,33,44,55],\n",
    "#              'weights': ['uniform','distance'],\n",
    "#              'leaf_size':[1,2,3,4,5,6,7,8,9,10,11,12]}]\n",
    "# -> {'leaf_size': 3, 'n_neighbors': 55, 'weights': 'distance'}\n",
    "# f1: 0.6115\n",
    "\n",
    "# param_grid = [{'n_neighbors': [1000,1500,2000,2500]}]\n",
    "# -> {'n_neighbors': 2500}\n",
    "\n",
    "knn_model = KNeighborsClassifier(n_neighbors=n_neighbors, weights=weights, leaf_size=leaf_size, n_jobs=-1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#norm of the penalty\n",
    "#‘l1’, ‘l2’, ‘elasticnet’, None\n",
    "penalty = 'l2'\n",
    "\n",
    "#inverse of regulation strength tau - how much do errors cost?\n",
    "C = 0.35\n",
    "\n",
    "#algorithm to compute optimization problem. Probs wont do much other than speed\n",
    "#‘lbfgs’, ‘liblinear’, ‘newton-cg’, ‘newton-cholesky’, ‘sag’, ‘saga’\n",
    "solver = 'sag'\n",
    "\n",
    "max_iter=500\n",
    "\n",
    "# param_grid = [{'penalty': ['l1','l2','elasticnet',None],\n",
    "#             'C': [0.25,0.5,0.75,1],\n",
    "#             'solver': ['lbfgs','newton-cg','newton-cholesky','sag','saga'],\n",
    "#             'max_iter': [1000,500,100]}]        \n",
    "# -> {'C': 0.5, 'max_iter': 500, 'penalty': 'l2', 'solver': 'sag'}\n",
    "# f1: 0.6916044272473679\n",
    "\n",
    "# param_grid = [{'C': [0.4,0.45,0.5,0.55,0.6],\n",
    "#             'solver': ['lbfgs','newton-cg','newton-cholesky','sag','saga'],\n",
    "#             'max_iter': [700,600,500,400,300]}]\n",
    "# -> {'C': 0.4, 'max_iter': 500, 'solver': 'sag'}\n",
    "# f1: 0.6915982039214981\n",
    "\n",
    "# param_grid = [{'C': [0.3,0.35,0.4,0.45]}]\n",
    "# -> {'C': 0.35}\n",
    "# f1: 0.6915982039214981\n",
    "\n",
    "regression_model = LogisticRegression(penalty=penalty, C=C, solver=solver, max_iter=max_iter, n_jobs=-1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try nonlinear regression by manipulating given features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to nonlinearly transform features!\n",
    "#np.sqrt, np.log, np.square\n",
    "f = np.sqrt\n",
    "\n",
    "merged_x = merged_x.apply(lambda x: f(x+1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#number of trees, int\n",
    "n_estimators=150\n",
    "\n",
    "#Criteria to determine split quality\n",
    "#“gini”, “entropy”, “log_loss”\n",
    "criterion = 'log_loss'\n",
    "\n",
    "#max depth of each tree, int or None for infinite\n",
    "max_depth = None\n",
    "\n",
    "#min #/% of samples to leave in a branch after a split\n",
    "min_samples_split = 0.25\n",
    "\n",
    "#min #/% of samples to be a leaf node\n",
    "min_samples_leaf = 0.4\n",
    "\n",
    "# param_grid = [{'n_estimators': [50,100,150],\n",
    "#             'criterion': ['gini','entropy','log_loss'],\n",
    "#             'max_depth':[None,10,15,20],\n",
    "#             'min_samples_split':[2,0.01,0.05,0.1,0.2],\n",
    "#             'min_samples_leaf':[2,0.01,0.05,0.1,0.2]}]\n",
    "# -> {'criterion': 'log_loss', 'max_depth': None, 'min_samples_leaf': 0.2, 'min_samples_split': 0.2, 'n_estimators': 150}\n",
    "# f1: 0.686935866983373\n",
    "\n",
    "# param_grid = [{'n_estimators': [125,150,175,200],\n",
    "#             'min_samples_split':[.1,.2,.3],\n",
    "#             'min_samples_leaf':[.1,.2,.3]}]\n",
    "# -> {'min_samples_leaf': 0.3, 'min_samples_split': 0.2, 'n_estimators': 150}\n",
    "# f1: 0.6944863744763707\n",
    "            \n",
    "# param_grid = [{'n_estimators': [140,145,150,155,160],\n",
    "#             'min_samples_split':[.15,.2,.25],\n",
    "#             'min_samples_leaf':[.25,.3,.35,.4]}]\n",
    "# -> {'min_samples_leaf': 0.4, 'min_samples_split': 0.25, 'n_estimators': 150}\n",
    "# f1: 0.6944863744763707\n",
    "\n",
    "# param_grid = [{'min_samples_split':[0.15,0.175,0.2,0.225,0.25,0.275,0.3]}]\n",
    "# -> 0.6944863744763707\n",
    "# {'min_samples_split': 0.25}\n",
    "\n",
    "randomforest_model = RandomForestClassifier(n_estimators=n_estimators, criterion=criterion, max_depth=max_depth, min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf, n_jobs=-1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting!\n",
    "make a model better! goes into *boosted_model*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "#model to build off of\n",
    "#estimator = model\n",
    "\n",
    "#max number of estimators to compute before terminating testing\n",
    "n_estimators = 20\n",
    "\n",
    "#weight applied to each classifier at each boosting iteration\n",
    "learning_rate = 1.0\n",
    "\n",
    "# param_grid = [{'n_estimators': [10,20,30],\n",
    "#             'learning_rate': [1,2,3]}]\n",
    "\n",
    "adaboosted_model = AdaBoostClassifier(estimator=regression_model, n_estimators=n_estimators, learning_rate=learning_rate)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "#loss function to use in boosting process\n",
    "#‘log_loss’, ‘auto’, ‘binary_crossentropy’/depreciated, ‘categorical_crossentropy’\n",
    "loss = 'log_loss'\n",
    "\n",
    "#shrinkage - multiplicative value for leaves\n",
    "learning_rate=0.24\n",
    "\n",
    "#maximum iterations for boosting process\n",
    "max_iter=1\n",
    "\n",
    "#maximum leaf nodes/tree\n",
    "max_leaf_nodes=2\n",
    "\n",
    "#max depth/tree or None for infinite\n",
    "max_depth = None\n",
    "\n",
    "#min samples to be a leaf\n",
    "min_samples_leaf=15\n",
    "\n",
    "#regularization parameter, 0 for none\n",
    "l2_regularization=0\n",
    "\n",
    "# param_grid = [{'learning_rate': [0.1,0.3,0.5,0.7,1],\n",
    "#             'max_iter':[50,60],\n",
    "#             'max_leaf_nodes':[25,30,35,40],\n",
    "#             'max_depth':[None,10,15,20],\n",
    "#             'min_samples_leaf':[20,0.01,0.05,0.1,0.2],\n",
    "#             'l2_regularization':[0,0.1,0.2]}] \n",
    "# -> f1: 0.67, {'l2_regularization': 0, 'learning_rate': 0.3, 'max_depth': None, 'max_iter': 50, 'max_leaf_nodes': 25, 'min_samples_leaf': 20}\n",
    "\n",
    "# param_grid = [{'learning_rate': [0.2,0.25,0.3,0.35,0.4],\n",
    "#             'max_iter':[30,40,50],\n",
    "#             'max_leaf_nodes':[10,15,20,25],\n",
    "#             'min_samples_leaf':[10,15,20,25,30]}] \n",
    "# -> f1: 0.66 {'learning_rate': 0.3, 'max_iter': 30, 'max_leaf_nodes': 10, 'min_samples_leaf': 15}\n",
    "\n",
    "# param_grid = [{'learning_rate': [0.25,0.275,0.3,0.325,0.35],\n",
    "#             'max_iter':[1,10,20,30],\n",
    "#             'max_leaf_nodes':[2,4,6,8,10],\n",
    "#             'min_samples_leaf':[12,13,14,15,16,17,18]}] \n",
    "# -> f1: 0.68 {'learning_rate': 0.275, 'max_iter': 1, 'max_leaf_nodes': 4, 'min_samples_leaf': 14}\n",
    "\n",
    "# param_grid = [{'learning_rate': [0.265,0.27,0.275,0.28,0.285],\n",
    "#             'max_leaf_nodes':[2,3,4,5,6],\n",
    "#             'min_samples_leaf':[13,14,15]}] \n",
    "# -> f1: 0.68 {'learning_rate': 0.27, 'max_leaf_nodes': 3, 'min_samples_leaf': 15}\n",
    "\n",
    "# param_grid = [{'learning_rate': [0.26,0.2665,0.267,0.2675,0.268,0.2685,0.269,0.2695,0.27,0.2705,0.271,0.2715,0.272,0.2725],\n",
    "#              'max_leaf_nodes':[2,3,4],\n",
    "#              'min_samples_leaf':[13,14,15,16,17],\n",
    "#              'max_iter':[0,1,2,3,4,5,6,7,8,9,10]}] \n",
    "# -> f1: 0.684 {'learning_rate': 0.269, 'max_iter': 3, 'max_leaf_nodes': 2, 'min_samples_leaf': 16}\n",
    "\n",
    "# param_grid = [{'learning_rate': [0.26,0.2665,0.267],\n",
    "#              'max_leaf_nodes':[4,5,6,7,8],\n",
    "#              'min_samples_leaf':[13,14,15],\n",
    "#              'max_iter':[3,4,5]}] \n",
    "# -> {'learning_rate': 0.26, 'max_iter': 3, 'max_leaf_nodes': 4, 'min_samples_leaf': 15}\n",
    "# f1: 0.6831502199369314\n",
    "\n",
    "# param_grid = [{'learning_rate': [0.24,0.245,0.25,0.255,0.26],\n",
    "#              'max_leaf_nodes':[1,2,3,4,5],\n",
    "#              'min_samples_leaf':[15,16,17,18],\n",
    "#              'max_iter':[1,2,3,4]}] \n",
    "# -> {'learning_rate': 0.24, 'max_iter': 1, 'max_leaf_nodes': 2, 'min_samples_leaf': 15}\n",
    "# f1: 0.694\n",
    "\n",
    "gradient_boosted_model = HistGradientBoostingClassifier(loss=loss, learning_rate=learning_rate, max_iter=max_iter, max_leaf_nodes=max_leaf_nodes, max_depth=max_depth, min_samples_leaf=min_samples_leaf, l2_regularization=l2_regularization)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging (for speed!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "#number of estimators\n",
    "n_estimators = 10\n",
    "\n",
    "bagged_model = BaggingClassifier(svm_model, max_features=1/n_estimators, n_estimators=n_estimators, n_jobs=-1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LATE FUSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "#which models to use for primary predictions\n",
    "estimators = [('forest',randomforest_model),('knn',knn_model),('dtree',dtree_model)]\n",
    "\n",
    "#which model to use for final prediction\n",
    "final_estimator = svm_model\n",
    "\n",
    "#k for cross validation for training of final_estimator\n",
    "cv = 10\n",
    "\n",
    "late_fused_model = StackingClassifier(estimators=estimators, final_estimator=final_estimator,cv=cv,n_jobs=-1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard scaler pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline([('scaler', StandardScaler()), ('model', knn_model)])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COMBINED NODE FOR TESTING AND PRINTING BC IM LAZY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "\tAVG, STDEV TEST RESULTS\n",
      "----------------------------------------\n",
      "\t\tMEAN\t\tSTDEV\n",
      "f1:       \t0.6945\t\t0.0001\n",
      "accuracy: \t0.532\t\t0.0001\n",
      "precision:\t0.532\t\t0.0001\n",
      "recall:   \t1.0\t\t0.0\n"
     ]
    }
   ],
   "source": [
    "#desired metrics\n",
    "scoring=['f1','accuracy','precision','recall']\n",
    "\n",
    "#number of \"folds\"\n",
    "k=10\n",
    "\n",
    "cv_results = cross_validate(randomforest_model, merged_x, merged_y, cv=k, scoring=scoring)\n",
    "\n",
    "# pretty print results\n",
    "hline = (\"-\"*40)\n",
    "\n",
    "#print means, stdevs\n",
    "print(hline+\"\\n\\tAVG, STDEV TEST RESULTS\\n\"+hline)\n",
    "print(\"\\t\\tMEAN\\t\\tSTDEV\")\n",
    "for m in scoring:\n",
    "        ind = (\"test_\"+m)\n",
    "        test = (m+\":\").ljust(10)\n",
    "        print(f\"{test}\\t{np.round(cv_results[ind].mean(),4)}\\t\\t{np.round(cv_results[ind].std(),4)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Fold testing \n",
    "\n",
    "(set model to variable *model*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#desired metrics\n",
    "scoring=['f1','accuracy','precision','recall']\n",
    "\n",
    "#number of \"folds\"\n",
    "k=10\n",
    "\n",
    "cv_results = cross_validate(model, merged_x, merged_y, cv=k, scoring=scoring)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty print results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hline = (\"-\"*40)\n",
    "\n",
    "#print means, stdevs\n",
    "print(hline+\"\\n\\tAVG, STDEV TEST RESULTS\\n\"+hline)\n",
    "print(\"\\t\\tMEAN\\t\\tSTDEV\")\n",
    "for m in scoring:\n",
    "        ind = (\"test_\"+m)\n",
    "        test = (m+\":\").ljust(10)\n",
    "        print(f\"{test}\\t{np.round(cv_results[ind].mean(),4)}\\t\\t{np.round(cv_results[ind].std(),4)}\")\n",
    "\n",
    "\n",
    "    ##print per test results (if you really want, uncomment)\n",
    "# print(\"\\n\")\n",
    "# print(hline+\"\\n\\tPER TEST RESULTS\\n\"+hline)\n",
    "# for i in range(k):\n",
    "#     print(f\"Test {i}:\")\n",
    "#     for m in scoring:\n",
    "#         ind = (\"test_\"+m)\n",
    "#         test = (m+\":\").ljust(10)\n",
    "#         print(f\"\\t{test} {np.round(cv_results[ind][i],5)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(merged_x, merged_y, test_size=0.5, random_state=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid Searching for Best Parameters\n",
    "\n",
    "must have parameters for variable in *params*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "grid_search = GridSearchCV(knn_model, param_grid=param_grid, scoring='f1', cv=10)\n",
    "grid_search.fit(x_train,y_train)\n",
    "print(grid_search.best_params_)\n",
    "print(grid_search.score(x_test,y_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Halving random grid search (for speed up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "\n",
    "hg_search = HalvingGridSearchCV(knn_model, param_grid=param_grid, scoring='f1', cv=10)\n",
    "hg_search.fit(x_train,y_train)\n",
    "print(hg_search.best_params_)\n",
    "print(hg_search.score(x_test,y_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False False False False False False False  True]\n",
      "[9 8 7 6 5 4 3 2 1]\n",
      "['Review_Count' '%_Verified' 'Total_Votes' 'Review_Avg_Sentiment'\n",
      " 'Summary_Avg_Sentiment' 'Review_Length' 'Summary_Length'\n",
      " 'Review_Percent_Uppercase' 'Summary_Percent_Uppercase']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "selector = RFECV(randomforest_model, step=1, cv=10) \n",
    "selector = selector.fit(merged_x,merged_y)\n",
    "print(selector.support_)\n",
    "print(selector.ranking_)\n",
    "print(selector.feature_names_in_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "\n",
    "print(merged_x.shape)\n",
    "new_x = SelectKBest(chi2, k='all')\n",
    "absx = merged_x.apply(lambda x: x+1) #selectKBest requires all positive values, sentiment analysis is between -1 and 1 -> 0 and 2 now!\n",
    "out = new_x.fit_transform(absx, merged_y)\n",
    "print(new_x.scores_)\n",
    "print(new_x.feature_names_in_)\n",
    "print(out.shape)\n",
    "\n",
    "# SELECTKBEST: [3.25559172e+02 5.59255454e-03 2.75958219e+01 3.66946669e+02\n",
    "#  3.42079412e+01 4.36649522e+01 1.85080492e+04 6.30872323e+01\n",
    "#  7.68359033e-01 3.83305432e-01 1.04268202e-02]\n",
    "# -['Review_Count' '%_Image' '%_Verified' 'Total_Votes'\n",
    "#  'Review_Avg_Sentiment' 'Summary_Avg_Sentiment' 'Review_Length'\n",
    "#  'Summary_Length' 'Summary_Percent_Uppercase' 'Review_Percent_Uppercase'\n",
    "#  'Actual_Awesomeness']\n",
    "#-> RFECV: [1 3 1 1 1 1 1 1 1 2]\n",
    "\n",
    "# SO: \n",
    "# Seems like %_image, actual_awesomeness do the least. Percent_uppercases do the next worst.\n",
    "# -> lets try cutting them out!\n",
    "\n",
    "#->\n",
    "# [  325.55917172    27.59582185   366.94666861    34.20794125\n",
    "#     43.6649522  18508.04915683    63.08723227]\n",
    "# ['Review_Count' '%_Verified' 'Total_Votes' 'Review_Avg_Sentiment'\n",
    "#  'Summary_Avg_Sentiment' 'Review_Length' 'Summary_Length']\n",
    "# -> RFECV2: [4 1 5 1 1 3 2]\n",
    "#"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = randomforest_model.fit(x_train, y_train).predict(x_test)\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test,y_pred)\n",
    "AUROC = metrics.auc(fpr, tpr)\n",
    "\n",
    "disp = metrics.RocCurveDisplay(fpr=fpr,tpr=tpr,roc_auc=AUROC)\n",
    "\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "ax.tick_params(axis='x', colors='white') \n",
    "ax.tick_params(axis='y', colors='white') \n",
    "ax.spines['left'].set_color('white') \n",
    "ax.spines['bottom'].set_color('white') \n",
    "\n",
    "plt.rcParams['figure.figsize'] = [10, 10]\n",
    "plt.xlabel(\"X\",fontsize=20, color='white')\n",
    "plt.ylabel(\"Y\",fontsize=20, color='white')\n",
    "plt.title(\"Area Under ROC Curve\", fontsize = 20, color='white')\n",
    "\n",
    "disp.plot(ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Prediction for Test Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading in test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.read_json('CDs_and_Vinyl/test1/review_test.json')\n",
    "test_y = pd.read_json('CDs_and_Vinyl/test1/product_test.json')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rerun Variable Analyses! then..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Again, combine all individual features into one dataFrame\n",
    "\n",
    "features = [reviewCount,percent_verified,total_votes,RavgSentiment,SavgSentiment,reviewlength,summarylength,RpercentCap,SpercentCap]\n",
    "\n",
    "test_merged_x = x['asin']\n",
    "for f in features:\n",
    "    test_merged_x = pd.merge(test_merged_x,f,'inner','asin')\n",
    "test_merged_x = test_merged_x.groupby(\"asin\").mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction\n",
    "predictions = randomforest_model.predict(test_merged_x) \n",
    "\n",
    "# Rejoin to create output\n",
    "test_y.insert(1, \"predictions\", predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y = pd.DataFrame(y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output predictions to file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output predictions to file!\n",
    "  \n",
    "#turn predictions into json\n",
    "json_str = test_y.to_json()\n",
    "\n",
    "#output json to file\n",
    "with open('predictions.json', 'w') as f:\n",
    "    f.write(json_str)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
